{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Kv2It0UYTBq7"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# TIME_FRAMES = 50\n",
    "BUFFER_SIZE = 100\n",
    "# AMOUNT_OF_FRAMES = TIME_FRAMES - BUFFER_SIZE + 1\n",
    "# AMOUNT_OF_VIDEOS = int(input(\"amount of videos: \"))\n",
    "\n",
    "WIDTH = 1280\n",
    "HEIGHT = 720\n",
    "\n",
    "frame_sq = []\n",
    "frame_sq_gray = []\n",
    "frame_sq_edges = []\n",
    "\n",
    "# dataset_no = input(\"starting number: \")\n",
    "video_count = 0\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "# rgb = np.ndarray((TIME_FRAMES - (TIME_FRAMES % BUFFER_SIZE), 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "UMWqnsH9TBrC"
   },
   "outputs": [],
   "source": [
    "def calculate_n_of_hsv(frame_sq,n):\n",
    "    sum_n = 0\n",
    "    var_n = 0\n",
    "    for frame in frame_sq:\n",
    "        var_n += np.var(frame[:,:,n])# / (np.mean(frame[:,:,n]) * 1\n",
    "        sum_n += np.sum(frame[:,:,n])\n",
    "\n",
    "    avg_n = sum_n/ (len(frame_sq) * frame_sq[0].shape[0] * frame_sq[0].shape[1])\n",
    "    return np.array([avg_n ,var_n])\n",
    "def calculate_comp(frame_sq):\n",
    "    sum_comp = 0\n",
    "    for i in range(len(frame_sq)):\n",
    "        if(i != 0):\n",
    "            sum_comp += compare_img(frame_sq[i], frame_sq[i-1])\n",
    "#         if(i == 1): print(compare_img(frame_sq[i], frame_sq[i-1]))\n",
    "    sum_comp /= (len(frame_sq) - 1)\n",
    "    return sum_comp\n",
    "def calculate_opticFlow(frame_sq):\n",
    "    prvs = cv2.resize(frame_sq[0],(0,0),fx=0.5,fy=0.5)\n",
    "    mag_sum = 0\n",
    "    ang_sum = 0\n",
    "    var_mag = 0\n",
    "    var_ang = 0\n",
    "    for i in range(1,len(frame_sq)):\n",
    "        frame2 = frame_sq[i]\n",
    "        next = cv2.resize(frame2, (0,0), fx=0.5, fy=0.5) \n",
    "        flow = cv2.calcOpticalFlowFarneback(prvs,next, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "        mag, ang = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
    "        mag = cv2.normalize(mag,None,0,1,cv2.NORM_MINMAX)\n",
    "        ang = np.absolute(cv2.normalize(ang,None,-1,1,cv2.NORM_MINMAX))\n",
    "        mag_sum = np.sum(mag)\n",
    "        ang_sum = np.sum(ang)\n",
    "        var_mag += np.var(mag)\n",
    "        var_ang += np.var(ang)\n",
    "    mag_sum /= (BUFFER_SIZE * WIDTH * HEIGHT)\n",
    "    ang_sum /= (BUFFER_SIZE * WIDTH * HEIGHT)\n",
    "    var_mag /= BUFFER_SIZE\n",
    "    var_ang /= BUFFER_SIZE\n",
    "    return np.array([mag_sum, ang_sum, var_mag, var_ang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_img(img1,img2):\n",
    "    img1 = normalize_gray(img1)\n",
    "    img2 = normalize_gray(img2)\n",
    "    diff = img1 - img2\n",
    "    m_norm = np.sum(abs(diff))\n",
    "#     return m_norm\n",
    "    return 0\n",
    "def normalize_frame(frame):\n",
    "    return frame/255\n",
    "\n",
    "def normalize_gray(frame):\n",
    "    rng = np.max(frame) - np.min(frame)\n",
    "    amin = np.min(frame)\n",
    "    return (frame-amin)/rng #range from [0,1]\n",
    "def createDataset(data):\n",
    "    dataset = data[0].copy()\n",
    "    data.remove(data[0])\n",
    "    for datum in data:\n",
    "        dataset = np.append(dataset, datum, axis = 1)\n",
    "    return dataset\n",
    "def createTSV(dataset,filename):\n",
    "    dataset = (dataset.copy()).astype(str)\n",
    "#     datasetName = \"dataset\" + str(dataset_no) + \".tsv\"\n",
    "    datasetName = filename + \".tsv\"\n",
    "    file = open(datasetName, \"w\")\n",
    "#     file.write(\"h\\ts\\tv\\tframe_edges_comp\\tframe_comp\\n\")\n",
    "    for i in range(dataset.shape[0]):\n",
    "        file.write(\"\\t\".join(dataset[i,:].tolist()))\n",
    "        file.write(\"\\n\")\n",
    "#         if(i + 1 != dataset.shape[0]):\n",
    "#             file.write(\"\\n\")\n",
    "    file.close()\n",
    "    return\n",
    "def createTSVLabel(dataset,filename):\n",
    "    dataset = (dataset.copy()).astype(str)\n",
    "#     datasetName = \"label\" + str(dataset_no) + \".tsv\"\n",
    "    datasetName = filename\n",
    "    file = open(datasetName, \"w\")\n",
    "    file.write(\"havg\\thv\\tsavg\\tsv\\tvavg\\tvv\\tmagavg\\tmagv\\tangavg\\tangv\\n\")\n",
    "    for i in range(dataset.shape[0]):\n",
    "        file.write(\"\\t\".join(dataset[i,:].tolist()))\n",
    "        file.write(\"\\n\")\n",
    "#         if(i + 1 != dataset.shape[0]):\n",
    "#             file.write(\"\\n\")\n",
    "    file.close()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Normalization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(dataset):\n",
    "    dataset = np.copy(dataset)\n",
    "    datasetNan = np.isnan(dataset).any(axis=1)\n",
    "    nans = []\n",
    "    for i in range(datasetNan.shape[0]):\n",
    "        if(datasetNan[i] == True):\n",
    "            nans.append(i)\n",
    "    dataset = dataset[~np.isnan(dataset).any(axis=1)]\n",
    "    max_values = np.max(dataset,axis=0)\n",
    "    min_values = np.min(dataset,axis=0)\n",
    "    ranges = max_values - min_values\n",
    "    dataset /= ranges\n",
    "    return (dataset,nans)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def createCSV(dataset):\n",
    "#     dataset = (dataset.copy()).astype(str)\n",
    "#     dataset_name = \"dataset\" + str(dataset_no) + \".csv\"\n",
    "#     csv = open(dataset_name, \"w\")\n",
    "#     columnTitleRow = \"havg,hv,savg,sv,vavg,vv,magavg,magv,angavg,angv\\n\"\n",
    "#     csv.write(columnTitleRow)\n",
    "#     for i in range(dataset.shape[0]):\n",
    "#         csv.write(\",\".join(dataset[i,:].tolist()))\n",
    "#         csv.write(\"\\n\")\n",
    "#     csv.close()\n",
    "def createCSV(dataset,filename):\n",
    "    dataset = (dataset.copy()).astype(str)\n",
    "#     dataset_name = \"dataset\" + str(dataset_no) + \".csv\"\n",
    "    dataset_name = filename + \".csv\"\n",
    "    csv = open(dataset_name, \"w\")\n",
    "    columnTitleRow = \"havg,hv,savg,sv,vavg,vv,magavg,magv,angavg,angv\\n\"\n",
    "    csv.write(columnTitleRow)\n",
    "    for i in range(dataset.shape[0]):\n",
    "        csv.write(\",\".join(dataset[i,:].tolist()))\n",
    "        csv.write(\"\\n\")\n",
    "    csv.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 564,
     "status": "error",
     "timestamp": 1528778942198,
     "user": {
      "displayName": "Chanokthorn Uerpairojkit",
      "photoUrl": "//lh3.googleusercontent.com/-lxOXQMW2xGM/AAAAAAAAAAI/AAAAAAAAAFM/6xYN4f5kB5I/s50-c-k-no/photo.jpg",
      "userId": "114952678521276212047"
     },
     "user_tz": -540
    },
    "id": "8oO3R5NyTBrI",
    "outputId": "63f9749a-562c-4191-a10e-99a47a62e6b4"
   },
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "frame_counter = 0\n",
    "buffer_counter = 0\n",
    "# filename = 'eye0.avi'\n",
    "# out = cv2.VideoWriter(filename, get_video_type(filename), 25, get_dims(cap, res))\n",
    "\n",
    "while(video_count < AMOUNT_OF_VIDEOS):\n",
    "    print(\"video \" + str(video_count) + \"...\")\n",
    "    for i in range (BUFFER_SIZE):\n",
    "        ret, frame = cap.read()\n",
    "        hsv = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "        frame_sq.append(hsv)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        frame_sq_gray.append(gray)\n",
    "        edges = cv2.Canny(gray,100,200)\n",
    "        frame_sq_edges.append(edges)\n",
    "        cv2.imshow(\"frame\",frame)\n",
    "        #\n",
    "        videos[video_count, i, :, :, :] = frame\n",
    "        #\n",
    "\n",
    "    h[video_count,:] = calculate_n_of_hsv(frame_sq,0)\n",
    "    s[video_count,:] = calculate_n_of_hsv(frame_sq,1)\n",
    "    v[video_count,:] = calculate_n_of_hsv(frame_sq,2)\n",
    "    frame_edges_comp[video_count,0] = calculate_comp(frame_sq_edges)\n",
    "    frame_comp[video_count,0] = calculate_comp(frame_sq_gray)\n",
    "    frame_sq = []\n",
    "    frame_sq_edges = []\n",
    "    frame_sq_gray = []\n",
    "    video_count += 1\n",
    "    tmp = input('enter anything to continue')\n",
    "#     (h[buffer_counter,:], h_histo) = calculate_n_of_hsv(frame_sq, 0)\n",
    "#     (s[buffer_counter,:], s_histo) = calculate_n_of_hsv(frame_sq, 1)\n",
    "#     (v[buffer_counter,:], v_histo) = calculate_n_of_hsv(frame_sq, 2)\n",
    "\n",
    "dataset = createDataset([h, s, v, frame_edges_comp, frame_comp])\n",
    "createTSV(dataset)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>STORE VIDEOS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'doit3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-db4075857d9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdirectory_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"directory name: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mBUFFER_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;31m# Cannot rely on checking for EEXIST, since the operating system\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'doit3'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "directory_name = input(\"directory name: \")\n",
    "os.makedirs(directory_name)\n",
    "\n",
    "BUFFER_SIZE = 100\n",
    "# AMOUNT_OF_VIDEOS = 70\n",
    "RECORD_AMOUNT = 20\n",
    "video_count = 50\n",
    "\n",
    "for i in range(RECORD_AMOUNT + 1):\n",
    "#     print(\"recording video \" + str(video_count) + \"...\")\n",
    "    sys.stdout.write(\"\\r\" + \"recording video \" + str(video_count) + \"...\")\n",
    "    sys.stdout.flush()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    WIDTH = int(cap.get(3))\n",
    "    HEIGHT = int(cap.get(4))\n",
    "    video_name = directory_name + \"/\" + str(video_count) + \".avi\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(directory_name + \"/\" + str(video_count) + '.avi',fourcc, 20.0, (640,480))\n",
    "    frame_counter = 0\n",
    "    while(True):\n",
    "        ret, frame = cap.read()\n",
    "        if (ret == True):\n",
    "            out.write(frame)\n",
    "#             cv2.imshow('frame',frame)\n",
    "            frame_counter += 1\n",
    "        if (frame_counter == BUFFER_SIZE): break\n",
    "    video_count += 1\n",
    "#     tmp = input(\"press enter to continue\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    tmp = input(\"press Enter\")\n",
    "video_count = 0\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ANALYZE VIDEOS + PREPROCESSING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "then\n",
      "video 68..."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "directory = input('enter folder name: ')\n",
    "vids = os.listdir(directory)\n",
    "tmp = np.arange(0,len(vids) - 1)\n",
    "videos = [name + '.avi' for name in (tmp.astype(str)).tolist()]\n",
    "videos = [directory + \"/\" + name for name in videos]\n",
    "video_count = 0\n",
    "AMOUNT_OF_VIDEOS = len(videos) - 1\n",
    "\n",
    "h = np.ndarray((AMOUNT_OF_VIDEOS, 2))\n",
    "s = np.ndarray((AMOUNT_OF_VIDEOS, 2))\n",
    "v= np.ndarray((AMOUNT_OF_VIDEOS, 2))\n",
    "opticFlow = np.ndarray((AMOUNT_OF_VIDEOS,4))\n",
    "\n",
    "print(\"then\")\n",
    "while(video_count < len(videos)-1):\n",
    "#     print(\"video \" + str(video_count) + \"...\")\n",
    "    sys.stdout.write(\"\\r\" + \"video \" + str(video_count) + \"...\")\n",
    "    sys.stdout.flush()\n",
    "    cap = cv2.VideoCapture(videos[video_count])\n",
    "    for i in range (BUFFER_SIZE):\n",
    "        ret, frame = cap.read()\n",
    "        hsv = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)  \n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) \n",
    "        \n",
    "        frame_sq.append(normalize_frame(hsv))\n",
    "        frame_sq_gray.append(normalize_frame(gray))\n",
    "        \n",
    "    h[video_count,:] = calculate_n_of_hsv(frame_sq,0)\n",
    "    s[video_count,:] = calculate_n_of_hsv(frame_sq,1)\n",
    "    v[video_count,:] = calculate_n_of_hsv(frame_sq,2)\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    opticFlow[video_count,:] = calculate_opticFlow(frame_sq_gray)\n",
    "    frame_sq = []\n",
    "    frame_sq_gray = []\n",
    "    video_count += 1\n",
    "    #     (h[buffer_counter,:], h_histo) = calculate_n_of_hsv(frame_sq, 0)\n",
    "#     (s[buffer_counter,:], s_histo) = calculate_n_of_hsv(frame_sq, 1)\n",
    "#     (v[buffer_counter,:], v_histo) = calculate_n_of_hsv(frame_sq, 2)\n",
    "\n",
    "datasetUnnormalized = createDataset([h,s,v,opticFlow])\n",
    "# datasetNormalized = normalize(datasetUnnormalized)\n",
    "# createTSV(datasetUnnormalized)\n",
    "# createCSV(datasetUnnormalized)\n",
    "# dataset_no = \"normalizedtest\"\n",
    "# createTSV(datasetNormalized)\n",
    "# createCSV(datasetNormalized)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doit3/0.avi', 'doit3/1.avi', 'doit3/2.avi', 'doit3/3.avi', 'doit3/4.avi', 'doit3/5.avi', 'doit3/6.avi', 'doit3/7.avi', 'doit3/8.avi', 'doit3/9.avi', 'doit3/10.avi', 'doit3/11.avi', 'doit3/12.avi', 'doit3/13.avi', 'doit3/14.avi', 'doit3/15.avi', 'doit3/16.avi', 'doit3/17.avi', 'doit3/18.avi', 'doit3/19.avi', 'doit3/20.avi', 'doit3/21.avi', 'doit3/22.avi', 'doit3/23.avi', 'doit3/24.avi', 'doit3/25.avi', 'doit3/26.avi', 'doit3/27.avi', 'doit3/28.avi', 'doit3/29.avi', 'doit3/30.avi', 'doit3/31.avi', 'doit3/32.avi', 'doit3/33.avi', 'doit3/34.avi', 'doit3/35.avi', 'doit3/36.avi', 'doit3/37.avi', 'doit3/38.avi', 'doit3/39.avi', 'doit3/40.avi', 'doit3/41.avi', 'doit3/42.avi', 'doit3/43.avi', 'doit3/44.avi', 'doit3/45.avi', 'doit3/46.avi', 'doit3/47.avi', 'doit3/48.avi', 'doit3/49.avi', 'doit3/50.avi', 'doit3/51.avi', 'doit3/52.avi', 'doit3/53.avi', 'doit3/54.avi', 'doit3/55.avi', 'doit3/56.avi', 'doit3/57.avi', 'doit3/58.avi', 'doit3/59.avi', 'doit3/60.avi', 'doit3/61.avi', 'doit3/62.avi', 'doit3/63.avi', 'doit3/64.avi', 'doit3/65.avi', 'doit3/66.avi', 'doit3/67.avi', 'doit3/68.avi', 'doit3/69.avi']\n"
     ]
    }
   ],
   "source": [
    "# print (datasetUnnormalized)\n",
    "print(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "(normalized,nans) = normalize(datasetUnnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(normalized)\n",
    "# print(nans)\n",
    "# print(normalize(datasetUnnormalized))\n",
    "createCSV(datasetUnnormalized,\"unnorm8\")\n",
    "createCSV(normalized,\"norm8\")\n",
    "createTSV(normalized,\"norm8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(nans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DATASET APPEND</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "def appendDatasets():\n",
    "    dataset = []\n",
    "    dataset_counter = 0\n",
    "    pathName = input(\"Path name: \")\n",
    "    while(True):\n",
    "        fileName = input(\"File name: \")\n",
    "        if(fileName == \"\"):\n",
    "            break\n",
    "        next_dataset = (genfromtxt(pathName + fileName, delimiter=','))[1:,:]\n",
    "        if(dataset_counter == 0):\n",
    "            dataset = next_dataset\n",
    "            dataset_counter += 1\n",
    "        else:\n",
    "            dataset = np.append(dataset, next_dataset, axis = 0)\n",
    "    return dataset\n",
    "def appendLabels():\n",
    "    labels = []\n",
    "    pathName = input(\"Path name: \")\n",
    "    while(True):\n",
    "        fileName = input(\"File name: \")\n",
    "        if(fileName == \"\"):\n",
    "            break\n",
    "        with open(pathName + fileName) as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            labels.append(line[:-1])\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = appendDatasets()\n",
    "createCSV(dts,\"Frankenstein\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = appendLabels()\n",
    "print(labels)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "ASD Vision measure.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
