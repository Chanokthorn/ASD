{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# The GPU id to use, usually either \"0\" or \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Kv2It0UYTBq7"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# TIME_FRAMES = 50\n",
    "BUFFER_SIZE = 100\n",
    "# AMOUNT_OF_FRAMES = TIME_FRAMES - BUFFER_SIZE + 1\n",
    "# AMOUNT_OF_VIDEOS = int(input(\"amount of videos: \"))\n",
    "\n",
    "WIDTH = 1280\n",
    "HEIGHT = 720\n",
    "\n",
    "frame_sq = []\n",
    "frame_sq_gray = []\n",
    "frame_sq_edges = []\n",
    "\n",
    "# dataset_no = input(\"starting number: \")\n",
    "video_count = 0\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "# rgb = np.ndarray((TIME_FRAMES - (TIME_FRAMES % BUFFER_SIZE), 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_img(img1,img2):\n",
    "    img1 = normalize_gray(img1)\n",
    "    img2 = normalize_gray(img2)\n",
    "    diff = img1 - img2\n",
    "    m_norm = np.sum(abs(diff))\n",
    "#     return m_norm\n",
    "    return 0\n",
    "def normalize_frame(frame):\n",
    "    return frame/255\n",
    "\n",
    "def normalize_gray(frame):\n",
    "    rng = np.max(frame) - np.min(frame)\n",
    "    amin = np.min(frame)\n",
    "    return (frame-amin)/rng #range from [0,1]\n",
    "def createDataset(data):\n",
    "    dataset = data[0].copy()\n",
    "#     dataset.remove(dataset[0])\n",
    "#     for datum in data:\n",
    "#         dataset = np.append(dataset, datum, axis = 1)\n",
    "#     result  = np.append(data[0],np.append(dataset[1],np.append(dataset[2],dataset[3],axis=1),axis=1),axis=1)\n",
    "    result = np.append(h,np.append(s,np.append(v,opticFlow,axis=1),axis=1),axis=1)\n",
    "    return result\n",
    "def createTSV(dataset,filename):\n",
    "    dataset = (dataset.copy()).astype(str)\n",
    "#     datasetName = \"dataset\" + str(dataset_no) + \".tsv\"\n",
    "    datasetName = filename + \".tsv\"\n",
    "    file = open(datasetName, \"w\")\n",
    "#     file.write(\"h\\ts\\tv\\tframe_edges_comp\\tframe_comp\\n\")\n",
    "    for i in range(dataset.shape[0]):\n",
    "        file.write(\"\\t\".join(dataset[i,:].tolist()))\n",
    "        file.write(\"\\n\")\n",
    "#         if(i + 1 != dataset.shape[0]):\n",
    "#             file.write(\"\\n\")\n",
    "    file.close()\n",
    "    return\n",
    "def createTSVLabel(dataset,filename):\n",
    "    dataset = (dataset.copy()).astype(str)\n",
    "#     datasetName = \"label\" + str(dataset_no) + \".tsv\"\n",
    "    datasetName = filename\n",
    "    file = open(datasetName, \"w\")\n",
    "    file.write(\"havg\\thv\\tsavg\\tsv\\tvavg\\tvv\\tmagavg\\tmagv\\tangavg\\tangv\\n\")\n",
    "    for i in range(dataset.shape[0]):\n",
    "        file.write(\"\\t\".join(dataset[i,:].tolist()))\n",
    "        file.write(\"\\n\")\n",
    "#         if(i + 1 != dataset.shape[0]):\n",
    "#             file.write(\"\\n\")\n",
    "    file.close()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Normalization</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from numpy import genfromtxt\n",
    "dataset = (genfromtxt(\"./unnormFrankensteinFullForm.csv\", delimiter=','))[1:,:]\n",
    "scaler = StandardScaler(copy=True)\n",
    "print(scaler.fit(dataset))\n",
    "def normalize(dataset):\n",
    "    datasetTmp = scaler.transform(dataset)\n",
    "    return datasetTmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createCSV(dataset,filename):\n",
    "    dataset = (dataset.copy()).astype(str)\n",
    "#     dataset_name = \"dataset\" + str(dataset_no) + \".csv\"\n",
    "    dataset_name = filename + \".csv\"\n",
    "    csv = open(dataset_name, \"w\")\n",
    "    columnTitleRow = \"havg,hv,savg,sv,vavg,vv,magavg,magv,angavg,angv\\n\"\n",
    "    csv.write(columnTitleRow)\n",
    "    for i in range(dataset.shape[0]):\n",
    "        csv.write(\",\".join(dataset[i,:].tolist()))\n",
    "        csv.write(\"\\n\")\n",
    "    csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>STORE VIDEOS</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import sys\n",
    "\n",
    "directory_name = input(\"directory name: \")\n",
    "os.makedirs(directory_name)\n",
    "\n",
    "BUFFER_SIZE = 100\n",
    "# AMOUNT_OF_VIDEOS = 70\n",
    "RECORD_AMOUNT = 20\n",
    "video_count = 91\n",
    "\n",
    "for i in range(RECORD_AMOUNT + 1):\n",
    "#     print(\"recording video \" + str(video_count) + \"...\")\n",
    "    sys.stdout.write(\"\\r\" + \"recording video \" + str(video_count) + \"...\")\n",
    "    sys.stdout.flush()\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    WIDTH = int(cap.get(3))\n",
    "    HEIGHT = int(cap.get(4))\n",
    "    video_name = directory_name + \"/\" + str(video_count) + \".avi\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter(directory_name + \"/\" + str(video_count) + '.avi',fourcc, 20.0, (640,480))\n",
    "    frame_counter = 0\n",
    "    while(True):\n",
    "        ret, frame = cap.read()\n",
    "        if (ret == True):\n",
    "            out.write(frame)\n",
    "#             cv2.imshow('frame',frame)\n",
    "            frame_counter += 1\n",
    "        if (frame_counter == BUFFER_SIZE): break\n",
    "    video_count += 1\n",
    "#     tmp = input(\"press enter to continue\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    tmp = input(\"press Enter\")\n",
    "video_count = 0\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>ANALYZE VIDEOS + PREPROCESSING</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-20b90f99f687>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mAMOUNT_OF_VIDEOS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAMOUNT_OF_VIDEOS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBUFFER_SIZE\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHEIGHT\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWIDTH\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mopticflow_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "directory = input('enter folder name: ')\n",
    "vids = os.listdir(directory)\n",
    "x = 0\n",
    "BUFFER_SIZE = 100\n",
    "tmp = np.arange(x,len(vids) - 1 + x)\n",
    "videos = [name + '.avi' for name in (tmp.astype(str)).tolist()]\n",
    "videos = [directory + \"/\" + name for name in videos]\n",
    "video_count = 0\n",
    "frame_count = 0\n",
    "AMOUNT_OF_VIDEOS = len(videos) - 1\n",
    "\n",
    "dataset = np.ndarray((AMOUNT_OF_VIDEOS, BUFFER_SIZE - 1, HEIGHT//2, WIDTH//2, 5))\n",
    "opticflow_buffer = []\n",
    "\n",
    "print(\"then\")\n",
    "while(video_count < len(videos)-1):\n",
    "# while(True):\n",
    "#     print(\"video \" + str(video_count) + \"...\")\n",
    "    sys.stdout.write(\"\\r\" + \"video \" + str(videos[video_count]) + \"...\")\n",
    "    sys.stdout.flush()\n",
    "    cap = cv2.VideoCapture(videos[video_count])\n",
    "    for i in range (BUFFER_SIZE):\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.resize(frame, (0,0), fx=0.5, fy=0.5)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        opticflow_buffer.append(gray)\n",
    "        if(len(opticflow_buffer) == 2):\n",
    "            hsv = cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)  \n",
    "            flow = cv2.calcOpticalFlowFarneback(opticflow_buffer[0],opticflow_buffer[1], None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "            del opticflow_buffer[0]\n",
    "            print(hsv.shape)\n",
    "            print(flow.shape)\n",
    "            data_frame = np.stack((hsv,flow),axis=-1)\n",
    "            dataset[video_count, frame_count, :, :, :]\n",
    "            frame_count += 1\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    opticflow_buffer = []\n",
    "    video_count += 1\n",
    "# datasetUnnormalized = createDataset([h,s,v,opticFlow])\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 10)\n",
      "['addition/71.avi', 'addition/72.avi', 'addition/73.avi', 'addition/74.avi', 'addition/75.avi', 'addition/76.avi', 'addition/77.avi', 'addition/78.avi', 'addition/79.avi', 'addition/80.avi', 'addition/81.avi', 'addition/82.avi', 'addition/83.avi', 'addition/84.avi', 'addition/85.avi', 'addition/86.avi', 'addition/87.avi', 'addition/88.avi', 'addition/89.avi']\n"
     ]
    }
   ],
   "source": [
    "datasetUnnormalized = createDataset([h,s,v,opticFlow])\n",
    "print(datasetUnnormalized.shape)\n",
    "print(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (normalized,nans) = normalize(datasetUnnormalized)\n",
    "(normalized) = normalize(datasetUnnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(normalized)\n",
    "# print(nans)\n",
    "# print(normalize(datasetUnnormalized))\n",
    "createCSV(datasetUnnormalized,\"unnormre1\")\n",
    "createCSV(normalized,\"normre1\")\n",
    "createTSV(normalized,\"normre1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(datasetUnnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.34587818e-01 1.17650179e+00 2.76209289e-01 1.95701144e+05\n",
      " 4.76514989e-01 6.75283781e+00 1.74247715e-05 3.90512085e-04\n",
      " 2.19686303e-03 1.01534326e-01]\n",
      "[[-4.01805330e-01 -1.00193358e+00 -4.81918523e-01  5.22244023e+04\n",
      "   6.36985515e-01 -1.02634940e+00  1.57668432e+00 -1.12283394e+00\n",
      "  -1.87308162e-01  1.19051603e-01]]\n"
     ]
    }
   ],
   "source": [
    "# print(normalize(datasetUnnormalized))\n",
    "# print(datasetUnnormalized)\n",
    "data = datasetUnnormalized[5,:]\n",
    "data[3] = 1.95701144e+05\n",
    "print(data)\n",
    "data = data.reshape(1,-1)\n",
    "print(scaler.transform(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = scaler.transform(datasetUnnormalized)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>DATASET APPEND</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import genfromtxt\n",
    "def appendDatasets():\n",
    "    dataset = []\n",
    "    dataset_counter = 0\n",
    "    pathName = input(\"Path name: \")\n",
    "    while(True):\n",
    "        fileName = input(\"File name: \")\n",
    "        if(fileName == \"\"):\n",
    "            break\n",
    "        next_dataset = (genfromtxt(pathName + fileName, delimiter=','))[1:,:]\n",
    "        if(dataset_counter == 0):\n",
    "            dataset = next_dataset\n",
    "            dataset_counter += 1\n",
    "        else:\n",
    "            dataset = np.append(dataset, next_dataset, axis = 0)\n",
    "    return dataset\n",
    "def appendLabels():\n",
    "    labels = []\n",
    "    pathName = input(\"Path name: \")\n",
    "    while(True):\n",
    "        fileName = input(\"File name: \")\n",
    "        if(fileName == \"\"):\n",
    "            break\n",
    "        with open(pathName + fileName) as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            labels.append(line[:-1])\n",
    "    return labels\n",
    "def saveLabels(label):\n",
    "#     pathName = input(\"Path name: \")\n",
    "    fileName = input(\"File name: \")\n",
    "    import pickle\n",
    "    pickle.dump(label,open(fileName + \".p\", \"wb\"))\n",
    "def makeTSVLabels(labels):\n",
    "    fileName = input(\"File name: \")\n",
    "    file = open(fileName + \".tsv\", \"w\")\n",
    "    for i in range(len(labels)):\n",
    "        file.write(labels[i] + \"\\n\")\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dts = appendDatasets()\n",
    "# createCSV(dts,\"normres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "createTSV(dts,\"unnormres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "createCSV(dts,\"unnormres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minn:  [1.68698948e-01 5.71359754e-01 7.26006841e-02 1.91583500e-01\n",
      " 2.30490743e-01 1.69076206e+00 1.60691440e-06 3.60041682e-04\n",
      " 7.00276490e-04 5.24942415e-02]\n",
      "rangee:  [1.88006912e-01 2.37081875e+00 5.25229358e-01 1.84091510e+01\n",
      " 3.98871483e-01 1.38006918e+01 9.30031806e-05 1.20997849e-03\n",
      " 4.86319761e-03 7.18616676e-02]\n"
     ]
    }
   ],
   "source": [
    "normdts = normalize(dts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "createCSV(normdts,\"normFrankensteinFullForm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "createTSV(dts,\"FrankensteinFullForm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['static', 'static', 'static', 'static', 'static', 'static', 'static', 'static', 'static', 'static', 'static', 'static', 'static', 'static', 'static', 'static', 'static', 'static', 'static', 'rot', 'rot', 'rot', 'rot', 'rot', 'rot', 'rot', 'rot', 'static', 'static', 'rot', 'rot', 'frot', 'frot', 'frot', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'unknown', 'light', 'unknown', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'move', 'move', 'move', 'move', 'move', 'move', 'move', 'move', 'move', 'move', 'move', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'frot', 'frot', 'frot', 'frot', 'frot', 'static', 'frot', 'move', 'move', 'move', 'move', 'move', 'move', 'move', 'move', 'move', 'move', 'move', 'move', 'move', 'light', 'light', 'light', 'light', 'unknown', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'light', 'unknown', 'light', 'light', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'dynamiclight', 'unknown', 'rot', 'rot', 'rot', 'rot', 'rot', 'rot', 'rot', 'rot', 'rot', 'rot', 'rot', 'frot', 'frot', 'frot', 'frot', 'frot', 'frot', 'frot', 'frot', 'frot', 'frot', 'frot', 'frot', '']\n"
     ]
    }
   ],
   "source": [
    "labels = appendLabels()\n",
    "print(labels)\n",
    "saveLabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=33641873408, available=27199905792, percent=19.1, used=8269729792, free=20736163840, active=7113564160, inactive=5019258880, buffers=653795328, cached=3982184448, shared=290054144, slab=381558784)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "psutil.virtual_memory()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "ASD Vision measure.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
